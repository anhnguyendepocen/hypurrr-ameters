---
title: "purrr_gridsearch_blog.Rmd"
author: "MDH"
date: "12/13/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 999) # disable scientific notation
```
# Hypurrr-ameter Search with Purrr and Future

#### See executable notebook of this post at this [Github repo](https://github.com/mrecos/hypurrr-ameters)!

I have had a thing lately with aggregating analyses into sequences of `purrr::map()` function calls and dense `tibbles`. If I see a loop - `map()` it. Have many outputs - well then put it in a `tibble`. Better yet, apply a sequence of functions over multiple model outputs, put them in a `tibble` and `map()` it! That is the basic approach I take here for modeling over a sequence of random hyperparameters (Hypurrr-ameters???); plus using `future` to do it in parallel. The idea for the code in this post came after an unsettled night of dreaming in the combinatoric magnitude of repeated K-folds CV over the hyperparameters for a multilayer perceptron. anyway...

"Correlated Hyperparameters"" - Zoroaster Clavis Artis 1738 (inspired by @vboykis #devart)
![](https://upload.wikimedia.org/wikipedia/commons/c/c0/ClavisArtis.Ms-2-27.Hortis.V3.034.jpg)


This post is based on the hyperparameter grid search example, but I am going to use it as a platform to go over some of the cool features of `purrr` that make it possible to put such an analysis in this `tibble` format.  Further, I hope this post gives people some examples that make the idea of `purrr` "click"; I know it took me some time to get there. By no means a primer on `purrr`, the text will hopefully make some connections between the ideas of `list-columns`, `purrr::map()` functions, and `purrr:nest()` to show off what I interpret as the Tidy-Purrr philosophy. The part about using `future` to parallelize this routine is presented towards the end.  If already know this stuff, then skip to the code examples at the end or see THIS REPO#%#%#%#%#.  However, if you are purrr-curious, give it a read and check out some of the amazing tutorials out in the wild.


*   __Objective__: Demonstrate an approach to randomly searching over model hyperparameters in parallel storing the results in a tibble.
*   __Pre-knowledge__: Beginner to Moderate R; introductory modeling concepts; Tidy/Purrr framework
*   __Software__: `R 3.4.0`, `tidyverse 1.2.1` (contains `tibble`, `dplyr`, `purrr`, `tidyr`, and `ggplot2`), `rsample 0.0.2`, `future 1.6.2`



![](https://dl.dropboxusercontent.com/s/9b46qii7e75n92r/heart_bar1.png?dl=0)



## Optimizing hyperparameters

Hyperparameters are the tuning knobs for statistical/machine learning models. Basic models such as linear regression and GLM family models don't typically have hyperparameters, but once you get into Ridge or Lasso regression and GAMs there are parts of the model that need tuning (e.g. penalties or smoothers). Methods such as Random Forest or Gradient Boosting, Neural Networks, and Gaussian Processes have even more tuning knobs and even less theory for how they should be set. Setting such hyperparameters can be a dark-art and require experience and a bit of faith. However, even with experience in these matters it is rarely clear what combination of hyperparameters will lead to the best out-of-sample prediction on a given dataset. For this reason, it is often desired to test over a range of hyperparameters to find the best pairing. 

The major issue of searching hyperparameters is the computational cost of searching a wide range of values to avoid local minima, testing over a fine enough grid to avoid over-shooting minima, and estimating variance via bootstrapping or K-folds CV. For example, a model with two hyperparameters tested over K = 10 folds CV for the values of each hyperparameter equates to 10 x 10 x 10 = 1,000 models! A third hyperparameter is another order of magnitude. Needless to say, this adds up fast. While a handful or algorithms and approaches have been developed to deal with this, the most common method is a basic grid-search or a random search across a sequence of hyperparameters. In this example, I employ a random search, but changing it to grid-search is simple.


![](http://purrr.tidyverse.org/logo.png)


## `Purrr::map()` and `tibble` 

To preform hyperparameter search in a table, we can use the power of [`purrr`](http://purrr.tidyverse.org/) to do the heavy lifting and the [`tibble`](http://tibble.tidyverse.org/) to store the results/computations. The `tibble` is much like the basic `data.frame` in many respects, but differing in some key ways that allow for the `purrr` idiom to really flourish. One of those differences is that the `tibble` is very accommodating to dealing with all sorts of data stuffed into the grid cells. Often referred to a list-column, this behavior allows the each cell in a column to store lists, which themselves can contain any R data structure like lists, data,frames, tibbles, data, functions, etc... The toy example below shows how typical numeric and character data are stored in `Col_A` and `Col_B` while `Col_C` is a list with three elements, one for each row, containing the character vector `"X","Y","Z"`. Finally, `Col_D` is a list that contains three elements (rows), each of which is also a list and contains three elements that are the XYZ character vector. Recursion!

```{r list_column, message=FALSE, warning=FALSE}
library("tidyverse")
list_col_example <- tibble(Col_A = c(1,2,3),
                           Col_B = c("A","B","C"),
                           Col_C = list(c("X","Y","Z")),
                           Col_D = list(Col_C))
print(list_col_example)
```


This recursive storage is really powerful! In this example, we take advantage of the list-column to store hyperparameter values that we then loop over to create and evaluate numerous models. Specifically, we will use the family of `map()` functions in `purrr` to iterate functions over elements of a list-column. If you are familiar with the world of `*apply()` functions, this is similar, but executed in a much more consistent manner and plays very well with other components of the [Tidyverse](https://www.tidyverse.org/). If you don't know much about `apply()` or `map()`, check out the [great tutorials](https://jennybc.github.io/purrr-tutorial/index.html) by [Jenny Bryan](https://twitter.com/JennyBryan) 
to see functional programming in action. Here are two examples of using `map()`. First, `map()` is used to apply the `paste()` function to the list-column `Col_C` along with the added argument of `collapse = ''`. The output, `Map_Col_1` is itself a list of character vectors just like `Col_C`. However, if we know the data type of the desired output, we can apply a type specific `map_chr()` function to cast the output to a character string in `Map_Col_2`. Pretty cool!


```{r map_example}
list_col_example <- list_col_example %>%
  mutate(Map_Col_1 = map(Col_C, paste, collapse = ''),
         Map_Col_2 = map_chr(Col_C, paste, collapse = ''))
print(list_col_example) 
```


![](http://tidyr.tidyverse.org/logo.png)

## Nesting a `tibble` with `tidyr::nest()`

The final piece of learning that builds up to the hyperparameter example is using the `nest()` function from the `tidyr` package to collapse table columns into list columns. This is similar in concept to grouping rows in a table based on values and returns a table of the groups within a new column. An example of how this works is well illustrated with the classic `iris` dataset. In `iris` you have four columns plant part measurments and one column with the iris flower species name; `Setosa`, `Versicolor`, and `Virginica`. There are 50 rows for each species. To collapse the data.frame into a `tibble` with one row for each species and one list-column containing a dataframe on the 50 examples of that species, you simply use the `nest()` function and tell it what column _not_ to include in the group; here it is `Species`

```{r nest_example}
head(iris)

nest_example <- iris %>%
  nest(-Species) %>%
  as.tibble()
print(nest_example)
```

cite: https://www.shareicon.net/plant-iris-blossom-nature-bloom-spring-floral-717702
![](https://www.shareicon.net/data/128x128/2016/02/12/717702_plant_512x512.png)

To close the loop on how you might use a nested `tibble` with `map()` for modeling, we can use the example above to show how you can run models on groups. We will use a simple linear regression wth `lm()` in a small helper function to model each species `Sepal.Length` by the other three measurments. (Note: this is just an example and modeling groups without pooling is probably a bad idea.). The use of a helper function is a really handy way to write a minimal bit of code that acts as a go-between from `map()` to your intended function. The very next line after the model is fit, we use `map_dbl()` to apply a root mean squared errors (RMSE) on the model fit to return a numeric value. The result of this is a `tibble` with a list-column names `model` that contians the entire `lm()` fit object and another column with a goodness-of-fit metric.

```{r iris_classification}

lm_helper <- function(data){
  lm1 <- lm(Sepal.Length ~ ., data = data)
}

nest_example2 <- nest_example %>%
  mutate(model = map(data, lm_helper),
         RMSE  = map_dbl(model, ~ sqrt(mean((.$residuals)^2))))
print(nest_example2)
```


## Mapping over Hypurrr-ameters

THe basis for the experiment in hyperparameter search is a tibble with a series of rows representignn random draws. In this case we are interested in mapping over the hyperparameters of `ntree` and `mtry`. It really doesn't matter much what these names mean as this can be applied to any model & hyperparameters. Check here to learn more about random forest check out [this post](https://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/). Suffice to say, these are two correlated settings of the randomforest algorithm that can have a big impact on performance. In order to build the basic table of hyperparameters to test over, we start a sequence of length `searches` and do random samples from sequences of `ntree` and `mtry`. The part of `ncol(data)-1` simpy returns the maximum number of predictor columns. See below code resulting in five rows of randomly selected pairs of `mtry` and `ntree`.

```{r packages, message=FALSE, warning=FALSE}
library("randomForest")
library("rsample")
library("viridis")
```

```{r base_table}
data <- MASS::Boston
searches <- 5 # number of random grid searches
max_trees <- 500
cv_folds <- 5

model_fits <- seq_len(searches) %>%
  tibble(
    id = .,
    ntree = sample(c(1,seq(25,max_trees,25)),length(id),replace = T),
    mtry  = sample(seq(1,ncol(data)-1,1),length(id),replace = T)
  )
print(model_fits)
```
```{r hide_for_later_use, include=FALSE}
sum_ntree <- sum(model_fits$ntree)
```

The next big step in the process is to `nest` each hyperparameter pair into its own small 1x2 `tibble`. The philosophy is that each of these hyperparemeter pair `tibbles` acts as the nugget that we keep applying the `map()` function over to conduct this whole analysis! In the code below we simmply exclude the `id` column from the nesting and assign the name `params` to the new aggregated column of hyperparameters

```{r nest_hyperparameters}
model_fits <- model_fits %>%
  nest(-id, .key = "params") 
print(model_fits)
```

Now the real work starts. First we have three new helper functions.

```{r helper_functions}
# splits CV objects into test, train, and parameter pairs
cv_helper <- function(data, params){
  d <- data %>%
    mutate(mtry  = params$mtry,
           ntree = params$ntree,
           train = map(splits, rsample::analysis),
           test  = map(splits, rsample::assessment)) %>%
    select(-splits)
}
# the main modeling helper. Trains and Tests RF model on each CV fold
rf_helper  <- function(folds){ 
  rf_model <- function(train,mtry,ntree,test){
    xtest  <- dplyr::select(test,-medv)
    ytest  <- test$medv
    m1 <- randomForest(medv ~., data=train,mtry=mtry,ntree=ntree,
                       xtest=xtest,ytest=ytest,keep.forest=TRUE)
  }
  # prediction function can go here and map in below if need be
  m <- folds %>%
    mutate(model = pmap(list(train,mtry,ntree,test),rf_model))
}
# Extracts test set MSE from RF Model and averages MSE from each ntree
MSE_helper <- function(model){
  err_helper <- function(rf){
    mse <- mean(rf$test$mse)
  }
  m2 <- model %>%
    mutate(mse = map_dbl(model, err_helper)) %>%
    dplyr::select(mse)
  mean(m2$mse)
}
```

Once we have the data shaped andhelper functions built, we can start to map our analysis over each row. First I use the `vfold_cv()` function from the `rsample` package to create a cross-validation folds (K = `cv_folds`). The slight trick is that I do this first and put it in a `list` so that the folds of each row (and `params` pair) is the same splits of the data. This is critical if you want to compare outcomes across hyperparameter pairs. Otherwise, each `params` pair is executed on a different set of data... and that is not good.

Next we use `map2()` to send two columns `splits` and `params` to the `cv_helper` function to divide the `split` object into `test` and `train` datasets, carrying over the hyperparameter pairs. Then the `tibble` in the `folds` column containg the `test`, `train`, and hyperparameter pair is sent to the `rf_helper` to fit the data on that hyperparameter pair for each of the folds in the CV split data sets. Noting that we save a step her because the `randomForest()` function allows you to not only train the model, but also test it at the same time! Supplying a `test` set of data and outcome results in the randomForest model object containing an out-of-sampe error rate for each model on each CV fold.

Finally the `cv_folds` number of models for each row (hyperparameter set) is mapped to the final helper function (MSE_helper) to average the Mean Squared Error (MSE) for each `ntree` of each model for each `cv_folds`. This happens for each row in `model_fits` equating to `ntree` x `cv_folds` x `searches` quantity of out-of-sample prediction evaluations. In this case that is ``r sum_ntree*cv_folds*searches`` evaluations.

```{r nonparllel_mapping}
model_fits <- model_fits %>%
   mutate(splits = list(rsample::vfold_cv(data,cv_folds)), # same resamples in each
          folds  = map2(splits, params, cv_helper),
          model  = map(folds, rf_helper),        # <- non-parallel version
          mse    = map_dbl(model, MSE_helper)) 
```

```{r results1}
mean(model_fits$mse)
print(dplyr::select(model_fits, id, model, mse))
```


results...

## Make it parallel

```{r}
library("future")
```





